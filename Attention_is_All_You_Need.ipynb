{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Attention-is-All-You-Need-Pytorch/blob/main/Attention_is_All_You_Need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2s48Vmoo9EB5"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchmetrics sacrebleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz8buKsjvA_w"
      },
      "source": [
        "## CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df355sdDrNSb"
      },
      "outputs": [],
      "source": [
        "# --- Data & Task Size ---\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "MODEL_CHOICE = \"YOUR_MODEL_NAME\" # For save path\n",
        "\n",
        "# --- Model Architecture Config (\"Transformer-Small\") ---\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "D_FF = 2048\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# --- Layer counts ---\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "NUM_DECODER_LAYERS = 6\n",
        "\n",
        "# --- Training Config ---\n",
        "TOKEN_LIMIT_PER_BATCH = 20000\n",
        "TARGET_TRAINING_STEPS = 100000\n",
        "VALIDATION_EVERY_N_STEPS = 4000\n",
        "PEAK_LEARNING_RATE = 5e-4\n",
        "WARMUP_STEPS = 4000\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# --- Regularization Config ---\n",
        "LABEL_SMOOTHING_EPSILON = 0.1\n",
        "RECONSTRUCTION_LOSS_WEIGHT = 0.1\n",
        "\n",
        "# --- Other Constants ---\n",
        "DRIVE_BASE_PATH = \"/content/YOUR_PREFERRED_PATH\"\n",
        "PREBATCHED_REPO_ID = \"Yujivus/wmt14-de-en-prebatched-w4\" # IMPORTANT\n",
        "ORIGINAL_BUCKETED_REPO_ID = \"Yujivus/wmt14-de-en-bucketed-w4\"\n",
        "MODEL_CHECKPOINT = \"Helsinki-NLP/opus-mt-de-en\" # We only use its tokenizer\n",
        "LABEL_SMOOTHING_EPSILON = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATALOADERS"
      ],
      "metadata": {
        "id": "W5l1HHRFXxPA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA5SqFzeMrpK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import math\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from torchmetrics.text import BLEUScore\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "from typing import List\n",
        "\n",
        "def set_seed(seed_value=5):\n",
        "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "SEED = 5\n",
        "set_seed(SEED)\n",
        "print(f\"Reproducibility seed set to {SEED}\")\n",
        "print(\"--- Loading Modernized Configuration ---\")\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    \"\"\"\n",
        "    DataLoader worker'larÄ± iÃ§in seed ayarlama fonksiyonu.\n",
        "    Her worker'Ä±n farklÄ± ama deterministik bir seed'e sahip olmasÄ±nÄ± saÄŸlar.\n",
        "    \"\"\"\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "print(\"âœ… PyTorch matmul precision set to 'high'\")\n",
        "\n",
        "# --- Device Setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "print(f\"Original tokenizer vocabulary size: {len(tokenizer)}\")\n",
        "\n",
        "current_vocab_size = len(tokenizer)\n",
        "\n",
        "# Pad to the nearest multiple of 8 for GPU Tensor Core efficiency\n",
        "padded_vocab_size = (current_vocab_size + 7) & (-8)\n",
        "num_added_tokens = padded_vocab_size - current_vocab_size\n",
        "\n",
        "if num_added_tokens > 0:\n",
        "    print(f\"Adding {num_added_tokens} dummy tokens to reach a multiple of 8.\")\n",
        "    tokenizer.add_tokens([f\"[PAD_{i+1}]\" for i in range(num_added_tokens)])\n",
        "\n",
        "# CRITICAL: This sets the correct, padded vocab size for the models\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "print(f\"New, padded vocabulary size for model: {VOCAB_SIZE}\")\n",
        "\n",
        "\n",
        "# DATA LOADING & PREPARATION\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "standard_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
        "\n",
        "class PreBatchedCollator:\n",
        "    def __init__(self, original_dataset_split):\n",
        "        self.original_dataset = original_dataset_split\n",
        "\n",
        "    def __call__(self, features: List[dict]) -> dict:\n",
        "        # 'features' will be a list of size 1, e.g., [{'batch_indices': [10, 5, 123]}]\n",
        "        batch_indices = features[0]['batch_indices']\n",
        "\n",
        "        # This returns a \"Dictionary of Lists\"\n",
        "        # e.g., {'input_ids': [[...], [...]], 'labels': [[...], [...]]}\n",
        "        dict_of_lists = self.original_dataset[batch_indices]\n",
        "\n",
        "        # --- THE FIX ---\n",
        "        # We must convert it to a \"List of Dictionaries\" for the standard collator.\n",
        "        # e.g., [{'input_ids': [...], 'labels': [...]}, {'input_ids': [...], 'labels': [...]}]\n",
        "        list_of_dicts = []\n",
        "        keys = dict_of_lists.keys()\n",
        "        num_samples = len(dict_of_lists['input_ids'])\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            list_of_dicts.append({key: dict_of_lists[key][i] for key in keys})\n",
        "        # --- END OF FIX ---\n",
        "\n",
        "        # Now, pass the correctly formatted data to the standard collator\n",
        "        return standard_collator(list_of_dicts)\n",
        "\n",
        "print(f\"Loading pre-batched dataset from: {PREBATCHED_REPO_ID}\")\n",
        "prebatched_datasets = load_dataset(PREBATCHED_REPO_ID)\n",
        "\n",
        "print(f\"Loading original samples from: {ORIGINAL_BUCKETED_REPO_ID}\")\n",
        "original_datasets = load_dataset(ORIGINAL_BUCKETED_REPO_ID)\n",
        "train_collator = PreBatchedCollator(original_datasets[\"train\"])\n",
        "\n",
        "# --- The New, Simple DataLoader ---\n",
        "# No more custom sampler!\n",
        "g = torch.Generator()\n",
        "g.manual_seed(SEED)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    prebatched_datasets[\"train\"],\n",
        "    batch_size=1,  # Each row is already a batch\n",
        "    shuffle=True,  # Shuffle the pre-calculated batches every epoch\n",
        "    num_workers=4,\n",
        "    collate_fn=train_collator,\n",
        "    pin_memory=True,\n",
        "    worker_init_fn=seed_worker,\n",
        "    generator=g,\n",
        ")\n",
        "\n",
        "# Validation loader remains the same, using the original data\n",
        "EVAL_BATCH_SIZE = 64\n",
        "val_dataloader = DataLoader(\n",
        "    original_datasets[\"validation\"],\n",
        "    batch_size=EVAL_BATCH_SIZE,\n",
        "    collate_fn=standard_collator,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    worker_init_fn=seed_worker,\n",
        "    generator=g,\n",
        ")\n",
        "\n",
        "print(\"\\n--- âœ… ULTIMATE DATALOADERS are ready ---\")\n",
        "print(f\"Train Dataloader is now a simple iterator over pre-calculated batches.\")\n",
        "\n",
        "# --- SANITY CHECK ---\n",
        "print(\"\\n--- Running Sanity Check on new DataLoader ---\")\n",
        "train_dataloader.generator.manual_seed(SEED) # Reset generator for check\n",
        "temp_iterator = iter(train_dataloader)\n",
        "print(\"Shapes of first 5 batches:\")\n",
        "for i in range(5):\n",
        "    batch = next(temp_iterator)\n",
        "    print(f\"  Batch {i+1}: input_ids shape = {batch['input_ids'].shape}\")\n",
        "print(\"--- Sanity Check Complete ---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS4JvJGRhClv"
      },
      "source": [
        "##  Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMhlM0YvO1A7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Injects positional information into the input embeddings.\"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"A standard two-layer feed-forward network with a ReLU activation.\"\"\"\n",
        "    def __init__(self, d_model: int, dff: int, dropout_rate: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.ffn(x)\n",
        "\n",
        "class StandardTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, num_heads, d_model, dff, vocab_size, max_length, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_length)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model, num_heads, dff, dropout, batch_first=True, norm_first=True # <-- THE FIX\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model, num_heads, dff, dropout, batch_first=True, norm_first=True # <-- THE FIX\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "        self.final_linear.weight = self.embedding.weight\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask, tgt_padding_mask, memory_key_padding_mask, tgt_mask):\n",
        "\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "        src_emb_pos = self.dropout(self.pos_encoder(src_emb))\n",
        "        tgt_emb_pos = self.dropout(self.pos_encoder(tgt_emb))\n",
        "\n",
        "        memory = self.encoder(src_emb_pos, src_key_padding_mask=src_padding_mask)\n",
        "        decoder_output = self.decoder(\n",
        "            tgt=tgt_emb_pos, memory=memory, tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "        return self.final_linear(decoder_output)\n",
        "\n",
        "\n",
        "    def create_masks(self, src, tgt):\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "        tgt_padding_mask = (tgt == tokenizer.pad_token_id)\n",
        "        # Creates a square causal mask for the decoder. This prevents any token from attending to future tokens. With this way model can not cheat.\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "            sz=tgt.size(1),\n",
        "            device=src.device,\n",
        "            dtype=torch.bool\n",
        "        )\n",
        "        return src_padding_mask, tgt_padding_mask, src_padding_mask, tgt_mask\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src: torch.Tensor, max_length: int, num_beams: int = 5) -> torch.Tensor:\n",
        "        self.eval()\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src_emb_pos = self.pos_encoder(src_emb)\n",
        "        memory = self.encoder(self.dropout(src_emb_pos), src_key_padding_mask=src_padding_mask)\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        memory = memory.repeat_interleave(num_beams, dim=0)\n",
        "        memory_key_padding_mask = src_padding_mask.repeat_interleave(num_beams, dim=0)\n",
        "\n",
        "        initial_token = tokenizer.pad_token_id\n",
        "        beams = torch.full((batch_size * num_beams, 1), initial_token, dtype=torch.long, device=src.device)\n",
        "\n",
        "        beam_scores = torch.zeros(batch_size * num_beams, device=src.device)\n",
        "        finished_beams = torch.zeros(batch_size * num_beams, dtype=torch.bool, device=src.device)\n",
        "        for _ in range(max_length - 1):\n",
        "            if finished_beams.all(): break\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(beams.size(1)).to(src.device)\n",
        "            tgt_emb = self.embedding(beams) * math.sqrt(self.d_model) # FIX HERE TOO\n",
        "            tgt_emb_pos = self.pos_encoder(tgt_emb)\n",
        "            decoder_output = self.decoder(tgt=self.dropout(tgt_emb_pos), memory=memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "            logits = self.final_linear(decoder_output[:, -1, :])\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            log_probs[:, tokenizer.pad_token_id] = -torch.inf\n",
        "            if finished_beams.any(): log_probs[finished_beams, tokenizer.eos_token_id] = 0\n",
        "            total_scores = beam_scores.unsqueeze(1) + log_probs\n",
        "            if _ == 0:\n",
        "                total_scores = total_scores.view(batch_size, num_beams, -1)\n",
        "                total_scores[:, 1:, :] = -torch.inf # Sadece ilk beam'in baÅŸlamasÄ±na izin ver\n",
        "                total_scores = total_scores.view(batch_size * num_beams, -1)\n",
        "            else:\n",
        "                total_scores = beam_scores.unsqueeze(1) + log_probs\n",
        "            total_scores = total_scores.view(batch_size, -1)\n",
        "            top_scores, top_indices = torch.topk(total_scores, k=num_beams, dim=1)\n",
        "            beam_indices = top_indices // log_probs.shape[-1]; token_indices = top_indices % log_probs.shape[-1]\n",
        "            batch_indices = torch.arange(batch_size, device=src.device).unsqueeze(1)\n",
        "            effective_indices = (batch_indices * num_beams + beam_indices).view(-1)\n",
        "            beams = beams[effective_indices]\n",
        "            beams = torch.cat([beams, token_indices.view(-1, 1)], dim=1)\n",
        "            beam_scores = top_scores.view(-1)\n",
        "            finished_beams = finished_beams | (beams[:, -1] == tokenizer.eos_token_id)\n",
        "        final_beams = beams.view(batch_size, num_beams, -1)\n",
        "        final_scores = beam_scores.view(batch_size, num_beams)\n",
        "        normalized_scores = final_scores / (final_beams != tokenizer.pad_token_id).sum(-1).float().clamp(min=1)\n",
        "        best_beams = final_beams[torch.arange(batch_size), normalized_scores.argmax(1), :]\n",
        "        self.train()\n",
        "        return best_beams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QGBtTvj6Jrp"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# --- Model Analysis & Parameter Counting ---\n",
        "# ==============================================================================\n",
        "from collections import defaultdict\n",
        "\n",
        "def count_parameters_correctly(model):\n",
        "    \"\"\"\n",
        "    Counts trainable parameters, correctly handling tied weights (e.g., embeddings).\n",
        "    \"\"\"\n",
        "    seen_params = set()\n",
        "    total_params = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            param_id = id(param)\n",
        "            if param_id not in seen_params:\n",
        "                seen_params.add(param_id)\n",
        "                total_params += param.numel()\n",
        "    return total_params\n",
        "\n",
        "# --- Instantiate the model to analyze it ---\n",
        "print(\"--- Analyzing Model Parameters ---\")\n",
        "model_to_analyze = StandardTransformer(\n",
        "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    d_model=D_MODEL,\n",
        "    dff=D_FF,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    max_length=MAX_LENGTH,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "# --- Perform the counting and display results ---\n",
        "correct_total = count_parameters_correctly(model_to_analyze)\n",
        "pytorch_naive_total = sum(p.numel() for p in model_to_analyze.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Trainable Parameters (Correctly Counted): {correct_total:,}\")\n",
        "print(f\"PyTorch's Naive Count (sum(p.numel())):        {pytorch_naive_total:,}\")\n",
        "if pytorch_naive_total != correct_total:\n",
        "    print(f\"Note: The naive count is higher due to double-counting the tied embedding weights.\")\n",
        "\n",
        "del model_to_analyze # Clean up memory\n",
        "print(\"--- Analysis Complete ---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd3AFTmhrCJq"
      },
      "source": [
        "## Functions (Loss, Eval etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te1qTyUKrDEd"
      },
      "outputs": [],
      "source": [
        "\n",
        "translation_loss_fn = nn.CrossEntropyLoss(\n",
        "    ignore_index=-100,  # We don't calculate loss for pad tokens. Pad tokens are replaced with -100 by DataCollatorForSeq2Seq.\n",
        "    label_smoothing=LABEL_SMOOTHING_EPSILON\n",
        ")\n",
        "def calculate_combined_loss(model_outputs, target_labels):\n",
        "    \"\"\"Calculates the loss based on the model's output structure.\"\"\"\n",
        "    logits = model_outputs\n",
        "    translation_loss = translation_loss_fn(logits.reshape(-1, logits.shape[-1]), target_labels.reshape(-1))\n",
        "    loss_dict = {'total': translation_loss.item()}\n",
        "    return translation_loss, loss_dict\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    \"\"\"Evaluates the model using beam search decoding.\"\"\"\n",
        "    bleu_metric = BLEUScore()\n",
        "\n",
        "\n",
        "    orig_model = getattr(model, '_orig_mod', model)\n",
        "    orig_model.eval()\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels']\n",
        "\n",
        "        generated_ids = orig_model.generate(input_ids, max_length=MAX_LENGTH, num_beams=5)\n",
        "\n",
        "        pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        labels[labels == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        bleu_metric.update(pred_texts, [[ref] for ref in ref_texts])\n",
        "\n",
        "    orig_model.train()\n",
        "    return bleu_metric.compute().item()\n",
        "\n",
        "def generate_sample_translations(model, device, sentences_de):\n",
        "    \"\"\"Generates and prints sample translations using beam search.\"\"\"\n",
        "    print(\"\\n--- Generating Sample Translations (with Beam Search) ---\")\n",
        "    orig_model = getattr(model, '_orig_mod', model)\n",
        "    orig_model.eval()\n",
        "\n",
        "    inputs = tokenizer(sentences_de, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    generated_ids = orig_model.generate(input_ids, max_length=MAX_LENGTH, num_beams=5)\n",
        "\n",
        "    translations = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    for src, out in zip(sentences_de, translations):\n",
        "        print(f\"  DE Source: {src}\")\n",
        "        print(f\"  EN Output: {out}\")\n",
        "        print(\"-\" * 20)\n",
        "    orig_model.train()\n",
        "\n",
        "sample_sentences_de_for_tracking = [\n",
        "    \"Eine Katze sitzt auf der Matte.\",\n",
        "    \"Ein Mann in einem roten Hemd liest ein Buch.\",\n",
        "    \"Was ist die Hauptstadt von Deutschland?\",\n",
        "    \"Ich gehe ins Kino, weil der Film sehr gut ist.\",\n",
        "]\n",
        "\n",
        "def init_other_linear_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # The 'is not' check correctly skips the final_linear layer,\n",
        "        # leaving its weights tied to the correctly initialized embeddings.\n",
        "        if m is not getattr(model, '_orig_mod', model).final_linear:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijTUk5dHu494"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyHZ1moluyA2"
      },
      "outputs": [],
      "source": [
        "# This script is controlled by the Master Control Panel in CELL 1.\n",
        "import torch\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    experiment_name = f\"{MODEL_CHOICE}_large_len{MAX_LENGTH}\"\n",
        "    SAVE_DIR = os.path.join(DRIVE_BASE_PATH, experiment_name, \"models\")\n",
        "    LOG_DIR = f\"/content/local_logs/{experiment_name}\"\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "    os.makedirs(LOG_DIR, exist_ok=True)\n",
        "    writer = SummaryWriter(LOG_DIR)\n",
        "    LAST_CHECKPOINT_PATH = os.path.join(SAVE_DIR, \"last.pt\")\n",
        "\n",
        "    print(\"--- LAUNCHING EXPERIMENT ---\")\n",
        "    print(f\"  Model Choice: {MODEL_CHOICE}\")\n",
        "    print(f\"  Max Sequence Length: {MAX_LENGTH}\")\n",
        "    print(f\"  Model Dimension: {D_MODEL}\")\n",
        "    print(f\"  Target Steps: {TARGET_TRAINING_STEPS}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    print(f\"--- Initializing StandardTransformer-Small (Baseline) ---\")\n",
        "    model = StandardTransformer(\n",
        "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
        "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
        "        num_heads=NUM_HEADS,\n",
        "        d_model=D_MODEL,\n",
        "        dff=D_FF,\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        max_length=MAX_LENGTH,\n",
        "        dropout=DROPOUT\n",
        "    )\n",
        "\n",
        "    print(\"--- Applying corrected weight initialization ---\")\n",
        "\n",
        "    # Because of weight tying, we are isolating and initializing embedding weights first.\n",
        "    # self.final_linear.weight is a pointer to model.embedding.weight\n",
        "\n",
        "    model.embedding.weight.data.normal_(mean=0.0, std=D_MODEL**-0.5)\n",
        "    print(\" Embedding layer initialized to N(0, d_model**-0.5)\")\n",
        "\n",
        "    # Then, we apply other weights\n",
        "    model.apply(init_other_linear_weights)\n",
        "    print(\" Other linear layers initialized to Xavier Uniform.\")\n",
        "\n",
        "    # --- FINAL SANITY CHECK ---\n",
        "    uncompiled_model = getattr(model, '_orig_mod', model)\n",
        "    final_weight_std = uncompiled_model.final_linear.weight.std().item()\n",
        "    target_std = D_MODEL**-0.5\n",
        "    print(\"\\n--- Verification ---\")\n",
        "    print(f\"Std deviation of final linear layer weights: {final_weight_std:.4f}\")\n",
        "    print(f\"Target std for embeddings:                 {target_std:.4f}\")\n",
        "    if abs(final_weight_std - target_std) < 0.01:\n",
        "        print(\" SUCCESS: Initialization appears correct.\")\n",
        "    else:\n",
        "        print(\" WARNING: Initialization might be incorrect. Standard deviations do not match.\")\n",
        "\n",
        "    model.to(device)\n",
        "    print(f\"Model '{MODEL_CHOICE}' initialized and moved to {device}.\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=PEAK_LEARNING_RATE,\n",
        "        betas=(0.9, 0.98),\n",
        "        eps=1e-9,\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    print(f\"Using AdamW optimizer with weight_decay={WEIGHT_DECAY}.\")\n",
        "    print(\"Optimizer LR is controlled by the Noam Scheduler.\")\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=WARMUP_STEPS,\n",
        "        num_training_steps=TARGET_TRAINING_STEPS\n",
        "    )\n",
        "    print(\"Using modern linear warmup, linear decay scheduler.\")\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    print(\"Using Automatic Mixed Precision (AMP) with GradScaler.\")\n",
        "    global_step = 0\n",
        "    best_bleu = 0.0\n",
        "    start_epoch = 0\n",
        "    if os.path.exists(LAST_CHECKPOINT_PATH):\n",
        "        print(f\"--- Resuming training from checkpoint: {LAST_CHECKPOINT_PATH} ---\")\n",
        "        checkpoint = torch.load(LAST_CHECKPOINT_PATH)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        global_step = checkpoint.get('global_step', 0)\n",
        "        best_bleu = checkpoint.get('best_bleu', 0.0)\n",
        "        start_epoch = checkpoint.get('epoch', 0)\n",
        "        print(f\"Resumed training from optimizer step {global_step}.\")\n",
        "        if 'rng_states' in checkpoint:\n",
        "            print(\"--- Restoring RNG states from checkpoint ---\")\n",
        "            rng_states = checkpoint['rng_states']\n",
        "            torch.set_rng_state(rng_states['torch_rng_state'])\n",
        "            np.random.set_state(rng_states['numpy_rng_state'])\n",
        "            random.setstate(rng_states['python_rng_state'])\n",
        "            torch.cuda.set_rng_state_all(rng_states['cuda_rng_states'])\n",
        "    else:\n",
        "        print(\"--- Starting training from scratch ---\")\n",
        "\n",
        "    print(\"--- Compiling model for optimized performance... ---\")\n",
        "    model = torch.compile(model, dynamic=True)\n",
        "\n",
        "    model.train()\n",
        "    progress_bar = tqdm(total=TARGET_TRAINING_STEPS, desc=\"Total Progress\", initial=global_step)\n",
        "    g = torch.Generator(); g.manual_seed(SEED)\n",
        "\n",
        "    for epoch in range(start_epoch, 100):\n",
        "        g.manual_seed(SEED + epoch)\n",
        "        train_dataloader.generator.manual_seed(SEED + epoch)\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "            if global_step >= TARGET_TRAINING_STEPS: break\n",
        "\n",
        "            # Standard PyTorch loop: zero grads, forward, backward, step\n",
        "            # --- 1. Zero Gradients ---\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            # --- 2. Data Preparation ---\n",
        "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "            labels = batch['labels'].to(device, non_blocking=True)\n",
        "\n",
        "            # Setting first token as pad token for decoder.\n",
        "            # Helsinki tokenizer uses pad token as start token.\n",
        "            decoder_start_token = torch.full((labels.shape[0], 1), tokenizer.pad_token_id, dtype=torch.long, device=device)\n",
        "            decoder_input_ids = torch.cat([decoder_start_token, labels[:, :-1]], dim=1)\n",
        "            decoder_input_ids[decoder_input_ids == -100] = tokenizer.pad_token_id\n",
        "            target_labels = labels\n",
        "            src_padding_mask, tgt_padding_mask, mem_key_padding_mask, tgt_mask = model.create_masks(input_ids, decoder_input_ids)\n",
        "            # We don't calculate attention for other padings.\n",
        "            tgt_padding_mask[:, 0] = False\n",
        "\n",
        "            # --- 3. Forward Pass with Autocast ---\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                model_outputs = model(src=input_ids, tgt=decoder_input_ids, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, memory_key_padding_mask=mem_key_padding_mask, tgt_mask=tgt_mask)\n",
        "                loss, loss_components = calculate_combined_loss(model_outputs, target_labels)\n",
        "\n",
        "            # --- 4. Backward Pass and Optimizer Step with Scaler --->\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Unscale gradients for clipping\n",
        "            scaler.unscale_(optimizer)\n",
        "            total_grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            global_step += 1\n",
        "            progress_bar.update(1)\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "            if global_step % 10 == 0:\n",
        "                lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "                writer.add_scalar('train/loss', loss.item(), global_step) #\n",
        "                writer.add_scalar('train/learning_rate', lr, global_step)\n",
        "                writer.add_scalar('train/gradient_norm', total_grad_norm.item(), global_step)\n",
        "\n",
        "                if 'trans' in loss_components:\n",
        "                    writer.add_scalar('train/loss_translation', loss_components['trans'], global_step)\n",
        "                if 'recon' in loss_components:\n",
        "                    writer.add_scalar('train/loss_reconstruction', loss_components['recon'], global_step)\n",
        "            progress_bar.set_postfix(\n",
        "                lr=f\"{lr:.2e}\",\n",
        "                grad_norm=f\"{total_grad_norm.item():.2f}\",\n",
        "                **loss_components\n",
        "            )\n",
        "\n",
        "            if global_step > 0 and global_step % VALIDATION_EVERY_N_STEPS == 0:\n",
        "                print(f\"\\n--- Validation at Optimizer Step {global_step} ---\")\n",
        "                bleu_score = evaluate(model, val_dataloader, device)\n",
        "                writer.add_scalar('validation/bleu', bleu_score, global_step)\n",
        "                print(f\"Validation BLEU Score: {bleu_score:.4f} (Best: {best_bleu:.4f})\")\n",
        "                generate_sample_translations(model, device, sample_sentences_de_for_tracking)\n",
        "\n",
        "                if bleu_score > best_bleu:\n",
        "                    best_bleu = bleu_score\n",
        "                    print(f\"ðŸŽ‰ New best BLEU score! Saving best model... ðŸŽ‰\")\n",
        "                    torch.save(getattr(model, '_orig_mod', model).state_dict(), os.path.join(SAVE_DIR, \"best.pt\"))\n",
        "                torch.save({\n",
        "                    'global_step': global_step,\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': uncompiled_model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'best_bleu': best_bleu,\n",
        "                    'rng_states': {\n",
        "                        'torch_rng_state': torch.get_rng_state(),\n",
        "                        'numpy_rng_state': np.random.get_state(),\n",
        "                        'python_rng_state': random.getstate(),\n",
        "                        'cuda_rng_states': torch.cuda.get_rng_state_all(),\n",
        "                    }\n",
        "                }, LAST_CHECKPOINT_PATH)\n",
        "                model.train()\n",
        "\n",
        "        if global_step >= TARGET_TRAINING_STEPS: break\n",
        "\n",
        "    progress_bar.close()\n",
        "    writer.close()\n",
        "    print(\"\\n--- Training finished ---\")\n",
        "\n",
        "    # --- Final Evaluation ---\n",
        "    print(\"\\n--- Running final evaluation on the best model ---\")\n",
        "    BEST_CHECKPOINT_PATH = os.path.join(SAVE_DIR, \"best.pt\")\n",
        "    if os.path.exists(BEST_CHECKPOINT_PATH):\n",
        "        uncompiled_model = getattr(model, '_orig_mod', model)\n",
        "        uncompiled_model.load_state_dict(torch.load(BEST_CHECKPOINT_PATH))\n",
        "\n",
        "        final_bleu = evaluate(model, val_dataloader, device)\n",
        "        print(f\"\\n{'*'*20} FINAL RESULTS {'*'*20}\")\n",
        "        print(f\"MODEL: {MODEL_CHOICE}\")\n",
        "        print(f\"Final Validation BLEU Score on best.pt: {final_bleu:.4f}\")\n",
        "        print(f\"{'*'*55}\")\n",
        "    else:\n",
        "        print(\"No 'best.pt' checkpoint found. Could not run final evaluation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqDiOyy18clU"
      },
      "outputs": [],
      "source": [
        "# TENSORBOARD VISUALIZATION\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "print(f\"--- Launching TensorBoard ---\")\n",
        "print(f\"Logs are being read from: {LOG_DIR}\")\n",
        "print(\"It may take a minute to load the data. Click the 'refresh' button in the UI if needed.\")\n",
        "\n",
        "%tensorboard --logdir {LOG_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI0-qVlWVVpx"
      },
      "source": [
        "## End"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "collapsed_sections": [
        "W5l1HHRFXxPA"
      ],
      "toc_visible": true,
      "mount_file_id": "1PpphTb75hsbt2SvX2auKCWNRh4NbwzWI",
      "authorship_tag": "ABX9TyPv7jqhjMfhbtizoiVLxbz+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}